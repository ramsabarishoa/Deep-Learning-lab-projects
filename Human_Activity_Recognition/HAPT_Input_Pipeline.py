"""HumanactivityHAPT_76.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rU0UkuNf35bceqagVfpJXwOzbruus-Gu
"""

from google.colab import drive
drive.mount('/content/drive')

#Unzip the dataset to a path in the drive
!unzip -uq "/content/drive/MyDrive/HAPT Data Set.zip" -d "/content/drive/MyDrive/Human_Activity"
print('Unzipped the contents to the drive')

import tensorflow as tf

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import os
import shutil
import re
from glob import glob

from scipy.stats import zscore
from absl import logging

from tensorflow import keras
from tensorflow.keras import layers

logging.set_verbosity(logging.INFO)

print('Importing Done')
print('Pandas Version:', pd.__version__)
print('Numpy Version:', np.__version__)
print('Tensorflow Version:', tf.__version__)

pd.set_option('display.max_rows', 5000)
pd.set_option('display.max_columns', 5000)
pd.set_option('display.width', 10000)

#The activties that are performed here are read from the .txt file
#There are 12 such activities
Activity_Labels = pd.read_csv('/content/drive/MyDrive/Human_Activity/activity_labels.txt', 
                              delimiter= '\s+', index_col=False, names=['label', 'activity'])
print(Activity_Labels)

#Subject IDs used for training
train_ids = pd.read_csv('/content/drive/MyDrive/Human_Activity/Train/subject_id_train.txt', 
                      sep=" ", header=None, names=['sub_train'])
print('The subject IDs used for training:',np.unique(train_ids['sub_train'].to_list()))
print('Total number of subjects used for training:', len(np.unique(train_ids['sub_train'].to_list())))

#Subject IDs used for testing
test_ids = pd.read_csv('/content/drive/MyDrive/Human_Activity/Test/subject_id_test.txt', 
                      sep=" ", header=None, names=['sub_train'])
print('The subject IDs used for training:',np.unique(test_ids['sub_train'].to_list()))
print('Total number of subjects used for training:', len(np.unique(test_ids['sub_train'].to_list())))

#Sample 3-axis accelerometer data for user01
acc = pd.read_csv('/content/drive/MyDrive/Human_Activity/RawData/acc_exp01_user01.txt', 
                  delimiter= '\s+', names=['x-axis', 'y-axis', 'z-axis'])
print(acc)

#Sample 3-axis gyroscope data for user01
gyc = pd.read_csv('/content/drive/MyDrive/Human_Activity/RawData/gyro_exp01_user01.txt', 
                  delimiter= '\s+', names=['x-axis', 'y-axis', 'z-axis'])
print(gyc)

#Labels dictionary
LABELS = {
    1: 'WALKING', 2: 'WALKING_UPSTAIRS', 3: 'WALKING_DOWNSTAIRS',  # 3 dynamic activities
    4: 'SITTING', 5: 'STANDING', 6: 'LYING',  # 3 static activities
    7: 'STAND_TO_SIT', 8: 'SIT_TO_STAND', 9: 'SIT_TO_LIE', 10: 'LIE_TO_SIT',
    11: 'STAND_TO_LIE', 12: 'LIE_TO_STAND',  # 6 postural Transitions
}

# Read the accelerometer and gyroscope data

def read_data(file_path, column_names):

    df = pd.read_csv(file_path, delimiter= '\s+',

                     header=None,

                     names=column_names)



    x = column_names[0]
    y = column_names[1]
    z = column_names[2]

    df[x] = df[x].apply(convert_to_float)
    df[y] = df[y].apply(convert_to_float)
    df[z] = df[z].apply(convert_to_float)
    df.dropna(axis=0, how='any', inplace=True)
    return df

def convert_to_float(x):
    try:
        return np.float(x)
    except:
        return np.nan

# Plot for user1

ax_acc = acc[["x-axis", "y-axis", "z-axis"]].plot(figsize=(20,4))
ax_acc.set_title('Accelerometer data for User01_Exp01')

ax_gyc = gyc[["x-axis", "y-axis", "z-axis"]].plot(figsize=(20,4))
ax_gyc.set_title('Gyroscope data for User01_Exp01')

#Plotting the 12 different activities for User01
df = pd.read_csv('/content/drive/MyDrive/Human_Activity/RawData/labels.txt', sep=" ", 
                 header=None, names=['Exp_number_ID', 'User_number_ID', 'Activity_number_ID', 'LStartp', 'LEndp'])
print(df.head())

#fig, axes = plt.subplots(nrows=4, ncols=3)
test_user01 = df[df['User_number_ID'] == 1]
test_user01 = test_user01.sort_values(by=['Activity_number_ID'])
test_user01 = test_user01.drop_duplicates(subset=['Activity_number_ID'], keep='first')
#print(test_user01)

for i in range(len(test_user01)):
  start_point = test_user01['LStartp'].iloc[i]
  end_point = test_user01['LEndp'].iloc[i]
  acc[["x-axis", "y-axis", "z-axis"]].iloc[start_point:end_point].plot(figsize=(10,4), 
                                                                       legend=True, title=Activity_Labels['activity'].iloc[i])

Raw_data_path = sorted(glob("/content/drive/My Drive/Human_Activity/RawData/*"))



#Separate out the gyro and accelerometer files

Raw_acc_path = Raw_data_path[0:61]
Raw_gyro_path = Raw_data_path[61:122]

labels_dataframe = pd.read_csv(Raw_data_path[122], delimiter=' ',
                              header=None,
                               names=['ExpID', 'UserID', 'ActivityID', 'StartTime', 'EndTime'])

sampling_freq = 50
noisy_rows = 250
features = 6
window_size = 250
window_shift = 125
batch_size = 16
shuffle_buffer = 200
posture_classes = 12

df_train = []
df_test = []
df_validation = []
labels_train = []
labels_test = []
labels_validation = []
df_raw_acc = []
rows_per_exp = []

user_id = 1
exp_id = 1
raw_acc_columns = ['acc_X', 'acc_Y', 'acc_Z']
raw_gyro_columns = ['gyro_X', 'gyro_Y', 'gyro_Z']

df_acc = read_data(Raw_data_path[user_id], raw_acc_columns)
df_acc = df_acc.iloc[noisy_rows:]
df_acc = df_acc.iloc[:-noisy_rows]
df_gyro = read_data(Raw_data_path[user_id + 61], raw_gyro_columns)
df_gyro= df_gyro.iloc[noisy_rows:]
df_gyro = df_gyro.iloc[:-noisy_rows]
df_signals = pd.concat([df_acc, df_gyro], axis=1)


for path_index in range(0, 61):

    user = int(Raw_data_path[path_index][-6:-4])
    exp_id = int(Raw_data_path[path_index][-13:-11])

    #Accelerometer ***********************************************************************
    df_acc= read_data(Raw_data_path[path_index], raw_acc_columns)
    #Removing the noisy rows
    df_acc = df_acc.iloc[noisy_rows:]
    df_acc = df_acc.iloc[:-noisy_rows]

    #Normalizing the data using Z-Score normalization

    df_acc['acc_X'] = (df_acc['acc_X'] - df_acc['acc_X'].mean()) / \
                                  df_acc['acc_X'].std(ddof=0)

    df_acc['acc_Y'] = (df_acc['acc_Y'] - df_acc['acc_Y'].mean()) / \
                                  df_acc['acc_Y'].std(ddof=0)

    df_acc['acc_Z'] = (df_acc['acc_Z'] - df_acc['acc_Z'].mean()) / \
                                  df_acc['acc_Z'].std(ddof=0)

    df_acc = df_acc.round({'acc_X': 4, 'acc_Y': 4, 'acc_Z': 4})

    #Gyroscope ***********************************************************************

    df_gyro  = read_data(Raw_data_path[path_index + 61], raw_gyro_columns)
    #Removing the noisy rows

    df_gyro  = df_gyro .iloc[noisy_rows:]
    df_gyro  = df_gyro .iloc[:-noisy_rows]

    #Normalize data using Z-Score normalization

    df_gyro ['gyro_X'] = (df_gyro ['gyro_X'] - df_gyro ['gyro_X'].mean()) / \
                                    df_gyro ['gyro_X'].std(ddof=0)

    df_gyro ['gyro_Y'] = (df_gyro ['gyro_Y'] - df_gyro ['gyro_Y'].mean()) / \
                                    df_gyro ['gyro_Y'].std(ddof=0)

    df_gyro ['gyro_Z'] = (df_gyro ['gyro_Z'] - df_gyro ['gyro_Z'].mean()) / \
                                    df_gyro ['gyro_Z'].std(ddof=0)

    df_gyro  = df_gyro .round({'gyro_X': 4, 'gyro_Y': 4, 'gyro_Z': 4}) #Format the data values

    #Combining the accelerometer and gyro data

    df_signals = pd.concat([df_acc , df_gyro ], axis=1) 
    df_signals = df_signals.to_numpy()

    #Unlabeled data is marked as zero
    labels = np.zeros(len(df_signals))

    for index, rows in labels_dataframe.iterrows():

        if rows['ExpID'] == exp_id:
            start = rows['StartTime']
            end = rows['EndTime']
            label_value = int(rows['ActivityID'])
            labels[start - noisy_rows:end - noisy_rows] = label_value

    if 1 <= user <= 21: #training samples
        labels_train.append(labels)
        for row in df_signals:
            row = row.reshape(1, 6).flatten()
            df_train.append(row)

    elif 22 <= user <= 27: #test samples
        labels_test.append(labels)
        for row in df_signals:
            row = row.reshape(1, 6).flatten()
            df_test.append(row)

    elif 28 <= user <= 30: #validation samples
        labels_validation.append(labels)
        for row in df_signals:
            row = row.reshape(1, 6).flatten()
            df_validation.append(row)        

labels_train = np.concatenate(labels_train).astype('int32')
labels_test = np.concatenate(labels_test).astype('int32')
labels_validation = np.concatenate(labels_validation).astype('int32')


df_train_subset = df_train[0:20000]
len_df = len(df_train_subset)

#One hot encoding for the labels

def one_hot_encoding_labels(n_labels):

    labels_one_hot = []

    x = np.zeros(posture_classes)

    for i in range(0, n_labels.size):
        x = np.zeros(posture_classes)
        if n_labels[i] == 1:
            x[0] = 1
            labels_one_hot.append(x)
        elif n_labels[i] == 2:
            x[1] = 1
            labels_one_hot.append(x)
        elif n_labels[i] == 3:
            x[2] = 1
            labels_one_hot.append(x)
        elif n_labels[i] == 4:
            x[3] = 1
            labels_one_hot.append(x)
        elif n_labels[i] == 5:
            x[4] = 1
            labels_one_hot.append(x)
        elif n_labels[i] == 6:
            x[5] = 1
            labels_one_hot.append(x)
        elif n_labels[i] == 7:
            x[6] = 1
            labels_one_hot.append(x)
        elif n_labels[i] == 8:
            x[7] = 1
            labels_one_hot.append(x)
        elif n_labels[i] == 9:
            x[8] = 1
            labels_one_hot.append(x)
        elif n_labels[i] == 10:
            x[9] = 1
            labels_one_hot.append(x)
        elif n_labels[i] == 11:
            x[10] = 1
            labels_one_hot.append(x)
        elif n_labels[i] == 12:
            x[11] = 1
            labels_one_hot.append(x)
        elif n_labels[i] == 0:
            labels_one_hot.append(x)

    return labels_one_hot

train_labels_one_hot = one_hot_encoding_labels(labels_train)
test_labels_one_hot = one_hot_encoding_labels(labels_test)
validation_labels_one_hot = one_hot_encoding_labels(labels_validation)

#Sliding window of size 250 is used here

def sliding_window(files, labels, window_size, window_overlap):
    list_files = []
    list_labels = []
    for i in range(0, int(len(files) / window_overlap) - 1):
        list_files.append(files[i * window_overlap:i * window_overlap + window_size])
        list_labels.append(labels[i * window_overlap:i * window_overlap + window_size])

    return list_files, list_labels

train_files, train_labels = sliding_window(df_train, train_labels_one_hot, window_size, window_shift)
test_files, test_labels = sliding_window(df_test, test_labels_one_hot, window_size, window_shift)
validation_files, validation_labels = sliding_window(df_validation, validation_labels_one_hot, window_size, window_shift)



def build_dataset(files, labels):

    ds = tf.data.Dataset.from_tensor_slices((files, labels))
    ds = ds.batch(batch_size)
    ds = ds.shuffle(shuffle_buffer)

    return ds

# Build the train, validation and test dataset
train_ds = build_dataset(train_files, train_labels)
test_ds = build_dataset(test_files, test_labels)
validation_ds = build_dataset(validation_files, validation_labels)

#Debug
print(train_ds)
print(validation_ds)
print(test_ds)