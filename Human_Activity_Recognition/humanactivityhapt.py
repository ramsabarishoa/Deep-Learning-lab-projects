# -*- coding: utf-8 -*-
"""HumanactivityHAPT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fFhOfSN3IjymDemusoGrwgroaqdnVWFT
"""

from google.colab import drive
drive.mount('/content/drive')

#Unzip the dataset to a path in the drive
!unzip -uq "/content/drive/MyDrive/HAPT Data Set.zip" -d "/content/drive/MyDrive/Human_Activity"
print('Unzipped the contents to the drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import os
import glob
import numpy as np
import pandas as pd
import shutil
import re
import matplotlib.pyplot as plt
from scipy.stats import zscore
import tensorflow as tf
from absl import logging
import tensorflow  as tf
import tensorflow.keras as keras
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from tensorflow import keras
from tensorflow.keras import layers
from glob import glob

logging.set_verbosity(logging.INFO)

print('Pandas Version:', pd.__version__)
print('Numpy Version:', np.__version__)

pd.set_option('display.max_rows', 5000)
pd.set_option('display.max_columns', 5000)
pd.set_option('display.width', 10000)

Activity_Labels = pd.read_csv('/content/drive/MyDrive/Human_Activity/activity_labels.txt', 
                              delimiter= '\s+', index_col=False, names=['label', 'activity'])
print(Activity_Labels)

#Train
train_ids = pd.read_csv('/content/drive/MyDrive/Human_Activity/Train/subject_id_train.txt', 
                      sep=" ", header=None, names=['sub_train'])
print('The subject IDs used for training:',np.unique(train_ids['sub_train'].to_list()))
print('Total number of subjects used for training:', len(np.unique(train_ids['sub_train'].to_list())))

#Test
test_ids = pd.read_csv('/content/drive/MyDrive/Human_Activity/Test/subject_id_test.txt', 
                      sep=" ", header=None, names=['sub_train'])
print('The subject IDs used for training:',np.unique(test_ids['sub_train'].to_list()))
print('Total number of subjects used for training:', len(np.unique(test_ids['sub_train'].to_list())))

#Accelerometer Train for user01
acc = pd.read_csv('/content/drive/MyDrive/Human_Activity/RawData/acc_exp01_user01.txt', 
                  delimiter= '\s+', names=['x-axis', 'y-axis', 'z-axis'])
print(acc)

#Gyroscope Train for user01
gyc = pd.read_csv('/content/drive/MyDrive/Human_Activity/RawData/gyro_exp01_user01.txt', 
                  delimiter= '\s+', names=['x-axis', 'y-axis', 'z-axis'])
print(gyc)

Sampling_Freq = 50
noisy_rows = 250
N_features = 6
N_WindowSize = 250
N_WindowShift = 125
N_prefetch = 8
batch_size = 16
N_shuffleBuffer = 200
N_classes = 12

df_train = []
df_test = []
df_validation = []
labels_train = []
labels_test = []
labels_validation = []
df_raw_acc = []
rows_per_exp = []

LABEL_NAMES = {
    1: 'WALKING', 2: 'WALKING_UPSTAIRS', 3: 'WALKING_DOWNSTAIRS',  # 3 dynamic activities
    4: 'SITTING', 5: 'STANDING', 6: 'LIYING',  # 3 static activities

    7: 'STAND_TO_SIT', 8: 'SIT_TO_STAND', 9: 'SIT_TO_LIE', 10: 'LIE_TO_SIT',
    11: 'STAND_TO_LIE', 12: 'LIE_TO_STAND',  # 6 postural Transitions
}

def read_data(file_path, column_names):
    df = pd.read_csv(file_path, delimiter=' ',
                     header=None,
                     names=column_names)

    x = column_names[0]
    y = column_names[1]
    z = column_names[2]
    df[x] = df[x].apply(convert_to_float)
    df[y] = df[y].apply(convert_to_float)
    df[z] = df[z].apply(convert_to_float)
    df.dropna(axis=0, how='any', inplace=True)
    return df

#Function to convert all accelerometer and gyroscope values to float
def convert_to_float(x):
    try:
        return np.float(x)
    except:
        return np.nan

Raw_data_path = sorted(glob("/content/drive/My Drive/RawData/*"))

#Seperate out gyro and accelerometer files
Raw_acc_path = Raw_data_path[0:61]
Raw_gyro_path = Raw_data_path[61:122]

labels_dataframe = pd.read_csv(Raw_data_path[122], delimiter=' ',
                               header=None,
                               names=['ExpID', 'UserID', 'ActivityID', 'StartTime', 'EndTime'])

user_id = 1
exp_id = 1
raw_acc_columns = ['acc_X', 'acc_Y', 'acc_Z']
raw_gyro_columns = ['gyro_X', 'gyro_Y', 'gyro_Z']




df_acc = read_data(Raw_data_path[user_id], raw_acc_columns)
df_acc = df_acc.iloc[noisy_rows:]
df_acc = df_acc.iloc[:-noisy_rows]
df_gyro = read_data(Raw_data_path[user_id + 61], raw_gyro_columns)
df_gyro= df_gyro.iloc[noisy_rows:]
df_gyro = df_gyro.iloc[:-noisy_rows]
df_signals = pd.concat([df_acc, df_gyro], axis=1)




for path_index in range(0, 61):
    user = int(Raw_data_path[path_index][-6:-4])
    exp_id = int(Raw_data_path[path_index][-13:-11])
    
    df_acc= read_data(Raw_data_path[path_index], raw_acc_columns)
    #Remove noisy rows
    df_acc = df_acc.iloc[noisy_rows:]
    df_acc = df_acc.iloc[:-noisy_rows]
    #Normalize data using Z-Score normalization
    df_acc['acc_X'] = (df_acc['acc_X'] - df_acc['acc_X'].mean()) / \
                                  df_acc['acc_X'].std(ddof=0)
    df_acc['acc_Y'] = (df_acc['acc_Y'] - df_acc['acc_Y'].mean()) / \
                                  df_acc['acc_Y'].std(ddof=0)
    df_acc['acc_Z'] = (df_acc['acc_Z'] - df_acc['acc_Z'].mean()) / \
                                  df_acc['acc_Z'].std(ddof=0)

    df_acc = df_acc.round({'acc_X': 4, 'acc_Y': 4, 'acc_Z': 4})
    

    df_gyro  = read_data(Raw_data_path[path_index + 61], raw_gyro_columns)
    df_gyro  = df_gyro .iloc[noisy_rows:]
    df_gyro  = df_gyro .iloc[:-noisy_rows]
    #Normalize data using Z-Score normalization
    df_gyro ['gyro_X'] = (df_gyro ['gyro_X'] - df_gyro ['gyro_X'].mean()) / \
                                    df_gyro ['gyro_X'].std(ddof=0)
    df_gyro ['gyro_Y'] = (df_gyro ['gyro_Y'] - df_gyro ['gyro_Y'].mean()) / \
                                    df_gyro ['gyro_Y'].std(ddof=0)
    df_gyro ['gyro_Z'] = (df_gyro ['gyro_Z'] - df_gyro ['gyro_Z'].mean()) / \
                                    df_gyro ['gyro_Z'].std(ddof=0)
    df_gyro  = df_gyro .round({'gyro_X': 4, 'gyro_Y': 4, 'gyro_Z': 4}) #round the data values
    #Concat accelerometer and gyro data
    df_signals = pd.concat([df_acc , df_gyro ], axis=1) 
    df_signals = df_signals.to_numpy()
    #Create labels array with all 0s (unlabelled data has to be labelled as 0)
    labels = np.zeros(len(df_signals))





    for index, rows in labels_dataframe.iterrows():
        if rows['ExpID'] == exp_id:
            start = rows['StartTime']
            end = rows['EndTime']
            label_value = int(rows['ActivityID'])
            labels[start - noisy_rows:end - noisy_rows] = label_value

    if 1 <= user <= 21: #training samples
        labels_train.append(labels)
        for row in df_signals:
            row = row.reshape(1, 6).flatten()
            df_train.append(row)

    elif 22 <= user <= 27: #test samples
        labels_test.append(labels)
        for row in df_signals:
            row = row.reshape(1, 6).flatten()
            df_test.append(row)

    elif 28 <= user <= 30: #validation samples
        labels_validation.append(labels)
        for row in df_signals:
            row = row.reshape(1, 6).flatten()
            df_validation.append(row)        

labels_train = np.concatenate(labels_train).astype('int32')
labels_test = np.concatenate(labels_test).astype('int32')
labels_validation = np.concatenate(labels_validation).astype('int32')

df_train_subset = df_train[0:20000]
len_df = len(df_train_subset)

def one_hot_labels(n_labels):
    labels_one_hot = []
    x = np.zeros(N_classes)
    for i in range(0, n_labels.size):
        x = np.zeros(N_classes)

        if n_labels[i] == 1:
            x[0] = 1
            labels_one_hot.append(x)

        elif n_labels[i] == 2:
            x[1] = 1
            labels_one_hot.append(x)

        elif n_labels[i] == 3:
            x[2] = 1
            labels_one_hot.append(x)

        elif n_labels[i] == 4:
            x[3] = 1
            labels_one_hot.append(x)

        elif n_labels[i] == 5:
            x[4] = 1
            labels_one_hot.append(x)

        elif n_labels[i] == 6:
            x[5] = 1
            labels_one_hot.append(x)

        elif n_labels[i] == 7:
            x[6] = 1
            labels_one_hot.append(x)

        elif n_labels[i] == 8:
            x[7] = 1
            labels_one_hot.append(x)

        elif n_labels[i] == 9:
            x[8] = 1
            labels_one_hot.append(x)

        elif n_labels[i] == 10:
            x[9] = 1
            labels_one_hot.append(x)

        elif n_labels[i] == 11:
            x[10] = 1
            labels_one_hot.append(x)

        elif n_labels[i] == 12:
            x[11] = 1
            labels_one_hot.append(x)

        elif n_labels[i] == 0:
            labels_one_hot.append(x)

    return labels_one_hot

train_labels_one_hot = one_hot_labels(labels_train)
test_labels_one_hot = one_hot_labels(labels_test)
validation_labels_one_hot = one_hot_labels(labels_validation)

def sliding_window(files, labels, window_size, window_overlap):
    list_files = []
    list_labels = []
    for i in range(0, int(len(files) / window_overlap) - 1):
        list_files.append(files[i * window_overlap:i * window_overlap + window_size])
        list_labels.append(labels[i * window_overlap:i * window_overlap + window_size])
    return list_files, list_labels

train_files, train_labels = sliding_window(df_train, train_labels_one_hot, N_WindowSize, N_WindowShift)
test_files, test_labels = sliding_window(df_test, test_labels_one_hot, N_WindowSize, N_WindowShift)
validation_files, validation_labels = sliding_window(df_validation, validation_labels_one_hot, N_WindowSize, N_WindowShift)

def build_dataset(files, labels):
    ds = tf.data.Dataset.from_tensor_slices((files, labels))
    ds = ds.batch(batch_size)
    ds = ds.shuffle(N_shuffleBuffer)
    ds = ds.prefetch(N_prefetch)
    return ds

train_ds = build_dataset(train_files, train_labels)
test_ds = build_dataset(test_files, test_labels)
validation_ds = build_dataset(validation_files, validation_labels)

print(test_ds)

inputs = keras.Input(shape=(N_WindowSize, N_features))
x = layers.LSTM(256,return_sequences=True)(inputs)
x = layers.Dropout(0.1)(x)
x = layers.LSTM(128,return_sequences=True)(x)
outputs = layers.Dense(12,activation='softmax')(x)


mdl = keras.Model(inputs=inputs, outputs=outputs, name='DR_model')


mdl.compile(loss=tf.keras.losses.categorical_crossentropy,
            optimizer='RMSProp',
            metrics=['accuracy'])

mdl.summary()
N_epochs = 30

history = mdl.fit(train_ds, epochs=N_epochs,
                  validation_data=validation_ds)