# -*- coding: utf-8 -*-
"""Main_7feb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12ut4h5DxpB6gMJG0qA-xQcyrrHZbWuTY
"""

import tensorflow as tf
import pandas as pd
import numpy as np
import zipfile
import glob, os
import sys
import matplotlib.pyplot as plt
import tensorflow.keras
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing.image import img_to_array, load_img

print(tf.__version__)

from google.colab import drive
drive.mount('/content/drive')

!unzip -uq "/content/drive/MyDrive/idrid.zip" -d "/content/drive/MyDrive"
print('Unzipped the contents to the drive')

batch_size = 32
img_ht = 256
img_wd = 256

pd.set_option('display.max_rows', 600)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 600)

np.set_printoptions(threshold=1000)

train_images = glob.glob("/content/drive/MyDrive/IDRID_dataset/images/train/*.jpg")
print('Total number of training images:',len(train_images))

df_train = pd.read_csv('/content/drive/MyDrive/IDRID_dataset/labels/train.csv')
df_train = df_train.drop_duplicates()
df_train = df_train.iloc[:, : 2]
#print(df_train.head())
df_train[['Retinopathy grade']].hist(figsize = (10, 5))
plt.title('Groundtruth Labels for Diabetic Retinopathy before binarization')
plt.xlabel('Label')
plt.ylabel('Number of Images')

df_train['Retinopathy grade'] = df_train['Retinopathy grade'].replace([0,1,2,3,4],[0,0,1,1,1])
df_train[['Retinopathy grade']].hist(figsize = (10, 5))
df_train = df_train.sample(frac=1).reset_index(drop=True)
plt.title('Groundtruth Labels for Diabetic Retinopathy after binarization')
plt.xlabel('Label')
plt.ylabel('Number of Images')

N_Training = round(len(df_train) * 0.8)
train = df_train[:N_Training]
validation = df_train[N_Training:]
print('---------------------------------------------------------')
print('Splitting the train samples into train and validation set')
print('---------------------------------------------------------')
print('Number of training samples:',len(train))
print('Number of validation samples:',len(validation))

label_0 = train[train['Retinopathy grade'] == 0]
label_1 = train[train['Retinopathy grade'] == 1]
print('Label 0:',len(label_0),'\n','Label 1:',len(label_1))

label_count_1, label_count_0 = train['Retinopathy grade'].value_counts()
label_0 = label_0.sample(label_count_1,replace=True)
df_train_sampled = pd.concat([label_0,label_1])
df_train_sampled = df_train_sampled.sample(frac=1,random_state=0)
print(len(label_0),len(label_1))
#print(df_train_sampled)
#df_train_sampled[['Retinopathy grade']].hist(figsize = (10, 5))

df_train_sampled['Image name'] = df_train_sampled['Image name'] + '.jpg'
validation['Image name'] = validation['Image name'] + '.jpg'

train_images_list = []
train_labels_list = []
for tname, tclass in df_train_sampled.itertuples(index=False):
    for ft in train_images:
      if os.path.basename(ft) == tname:
        #print(fp,iname,iclass)
        train_images_list.append(ft)
        train_labels_list.append(tclass)

val_images_list = []
val_labels_list = []
for vname, vclass in validation.itertuples(index=False):
  for fv in train_images:
      if os.path.basename(fv) == vname:
        #print(fv,vname,vclass)
        val_images_list.append(fv)
        val_labels_list.append(vclass)

val_img = np.array([img_to_array(load_img(img, target_size=(256, 256)))for img in val_images_list]).astype('float32')
val_labels_list = np.array(val_labels_list)

print(len(train_images_list),len(train_labels_list))
print(len(val_images_list), len(val_labels_list))

def _parse_function(image, label):
  img_train = tf.io.read_file(image)
  img_decoded = tf.io.decode_jpeg(img_train)
  img_cropped = tf.image.central_crop(img_decoded, central_fraction=0.95)
  img_cropped_bound = tf.image.crop_to_bounding_box(img_cropped, 0 , 0 , target_height = 2700, target_width = 3580)
  image_cast = tf.cast(img_cropped_bound, tf.float32) 
  image_cast = image_cast / 255.0
  image_resized = tf.image.resize(image_cast,size=(img_ht,img_wd))
  return image_resized, label

def build_dataset(images, labels):
  AUTOTUNE = tf.data.experimental.AUTOTUNE
  dataset = tf.data.Dataset.from_tensor_slices((images, labels))
  dataset = dataset.cache()
  dataset = dataset.map(_parse_function)
  dataset = dataset.batch(len(images))
  dataset = dataset.prefetch(AUTOTUNE)
  return dataset

train_dataset = build_dataset(train_images_list, train_labels_list)
#Debug
print(tf.data.experimental.cardinality(train_dataset).numpy())

def to_train_datagen():
  for image, label in train_dataset:
    image_matrix = image.numpy()
    label_matrix = label.numpy()
    print(image_matrix.shape)
    print(label_matrix.shape)

  return image_matrix, label_matrix

train_image_array, train_label_array = to_train_datagen()

# Create train generator.
train_datagen = ImageDataGenerator(rotation_range=30, 
                                   width_shift_range=0.2,
                                   height_shift_range=0.2, 
                                   zoom_range=0.001,
                                   horizontal_flip = 'true')
train_generator = train_datagen.flow(train_image_array, train_label_array, shuffle=True, batch_size=batch_size)

# Create validation generator
val_datagen = ImageDataGenerator(rescale = 1./255)
val_generator = val_datagen.flow(val_img, val_labels_list, shuffle=False, 
                                   batch_size=batch_size)

from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers

model = Sequential()
model.add(Conv2D(8, kernel_size=(3, 3),strides =2 ,
                 activation='relu', 
                 input_shape=(256,256,3)))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(16, kernel_size=(3, 3),strides =2,
                 activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.4))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax'))

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

print(model.summary())

epochs=150
history = model.fit(
  train_generator,
  validation_data=val_generator,
  epochs=epochs
  
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(30, 10))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

#Prediction
#Loading test images 
test_images = glob.glob("/content/drive/MyDrive/IDRID_dataset/images/test/*.jpg")
#print(len(test_images))
df_test = pd.read_csv('/content/drive/MyDrive/IDRID_dataset/labels/test.csv')
df_test['Retinopathy grade'] = df_test['Retinopathy grade'].replace([0,1,2,3,4],[0,0,1,1,1])
df_test['Image name'] = df_test['Image name'] + '.jpg'
df_test = df_test.drop_duplicates()
df_test = df_test.iloc[:, : 2]
#print(df_test)
predicted_label_list = []
for iname,iclass in df_test.itertuples(index=False):
    for file in test_images:
      if os.path.basename(file) == iname:
        img = tf.io.read_file(file)
        img = tf.io.decode_jpeg(img)
        img = tf.cast(img,tf.float32) / 255
        img = tf.image.resize_with_pad(img,img_ht,img_wd,antialias=True)
        img = tf.reshape(img, [1,256,256,3])
        x = model.predict(img)
        predicted_label = np.argmax(x)
        predicted_label_list.append(predicted_label)

df_test['Predicted Class'] = predicted_label_list
df_test['Result'] = np.where(df_test['Retinopathy grade'] == df_test['Predicted Class'], 'Correct Prediction', 'Incorrect Prediction')
#print(df_test)

df_test['Result'].value_counts()

from sklearn import metrics
from sklearn.metrics import confusion_matrix
import seaborn as sb
cm = confusion_matrix(df_test['Retinopathy grade'],df_test['Predicted Class'])
plt.figure(figsize = (10,5))
plt.title('Confusion Matrix')
sb.heatmap(cm, cmap="Blues", annot=True,annot_kws={"size": 16})
print('Test Accuracy:',metrics.accuracy_score(df_test['Retinopathy grade'], df_test['Predicted Class']))

from tensorflow import keras

# Display
from IPython.display import Image
import matplotlib.pyplot as plt
import matplotlib.cm as cm

import cv2
from google.colab.patches import cv2_imshow

print(model.input)
print(model.output)

activation_grad_model = tf.keras.models.Model([model.input], [model.get_layer('conv2d_1').output, model.output])

layers_name = ['conv2d_1']
#layers_name = ['conv2d_2']
#layers_name = ['conv2d_3']
outputs = [
    layer.output for layer in model.layers
    if layer.name in layers_name
]
print(outputs)
for ilayer, layer in enumerate(model.layers):
    print("{:3.0f} {:10}".format(ilayer, layer.name))
	
#GradCam Visualization for few images
img_path = '/content/drive/MyDrive/IDRID_dataset/images/test/IDRiD_038.jpg'
img = tf.io.read_file(img_path)
img = tf.io.decode_jpeg(img)
img = tf.cast(img,tf.float32) / 255
img = tf.image.resize_with_pad(img,img_ht,img_wd,antialias=True)

img = tf.reshape(img, [1,256,256,3])
x = model.predict(img)
predicted_label = np.argmax(x)
print(type(img))
print('Actual Label for the image', df_test['Image name'].iloc[18], 'is', df_test['Retinopathy grade'].iloc[18])
print('Predicted Label:', predicted_label)

with tf.GradientTape() as tape:
      conv_outputs, predictions = activation_grad_model(img)
      loss = predictions[:, predicted_label]

output = conv_outputs[0]
grads = tape.gradient(loss,conv_outputs)[0]

gate_f = tf.cast(output > 0, 'float32')
gate_r = tf.cast(grads > 0, 'float32')
guided_grads = tf.cast(output > 0, 'float32') * tf.cast(grads > 0, 'float32') * grads

weights = tf.reduce_mean(guided_grads, axis=(0, 1))
weights_gradcam = tf.reduce_mean(grads, axis=(0, 1))

cam = np.ones(output.shape[0:2], dtype=np.float32)
cam_gradcam = np.ones(output.shape[0:2], dtype=np.float32)

for index, w in enumerate(weights):
  cam += w * output[:,:,index]

for index, w in enumerate(weights_gradcam):
  cam_gradcam += w * output[:,:,index]

cam = cv2.resize(cam.numpy(), (256, 256))
cam_gradcam = cv2.resize(cam_gradcam.numpy(), (256, 256))

cam = np.maximum(cam, 0)
cam_gradcam = np.maximum(cam_gradcam, 0)

heatmap = (cam - cam.min()) / (cam.max() - cam.min())
heatmap_gradcam = (cam_gradcam - cam_gradcam.min()) / (cam_gradcam.max() - cam_gradcam.min())


img = tf.reshape(img, (256,256,3))
img = np.asarray(img).astype('float32')
print(img.shape, img.dtype)


cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)
output_image = cv2.addWeighted(cv2.cvtColor(img.astype('uint8'), cv2.COLOR_RGB2BGR), 0.5, cam, 1, 0)
  
cam_gradcam = cv2.applyColorMap(np.uint8(255*heatmap_gradcam), cv2.COLORMAP_JET)
output_image_gradcam = cv2.addWeighted(cv2.cvtColor(img.astype('uint8'), cv2.COLOR_RGB2BGR), 0.5, cam_gradcam, 1.0, 0)


b, g, r = cv2.split(output_image)
output_image = cv2.merge([r, g, b])

b, g, r = cv2.split(output_image_gradcam)
output_image_gradcam = cv2.merge([r, g, b])

plt.figure(figsize=(20,20))
plt.subplot(1, 3, 1)
plt.title('Test Image with Diabetic Retinopathy')
plt.imshow(img)

plt.subplot(1, 3, 2)
plt.title('Grad-CAM ')
plt.imshow(output_image)

plt.subplot(1, 3, 3)
plt.title('Guided Grad-CAM')
plt.imshow(output_image_gradcam)
