# -*- coding: utf-8 -*-
"""drd_7475_perfect__22_Dec_Uploaed (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ETARkK-brTN3DRgWcKCvKRTUt9s9KirV
"""

import tensorflow as tf
import pandas as pd
import numpy as np
import zipfile
import glob, os
import sys
import matplotlib.pyplot as plt

from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing.image import img_to_array, load_img

print(tf.__version__)

from google.colab import drive
drive.mount('/content/drive')

!unzip -uq "/content/drive/MyDrive/idrid.zip" -d "/content/drive/MyDrive"
print('Unzipped the contents to the drive')

batch_size = 32
img_ht = 256
img_wd = 256

pd.set_option('display.max_rows', 600)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 600)

np.set_printoptions(threshold=1000)

train_images = glob.glob("/content/drive/MyDrive/IDRID_dataset/images/train/*.jpg")
print('Total number of training images:',len(train_images))

df_train = pd.read_csv('/content/drive/MyDrive/IDRID_dataset/labels/train.csv')
df_train = df_train.drop_duplicates()
df_train = df_train.iloc[:, : 2]
#print(df_train.head())
df_train[['Retinopathy grade']].hist(figsize = (10, 5))
plt.title('Groundtruth Labels for Diabetic Retinopathy before binarization')
plt.xlabel('Label')
plt.ylabel('Number of Images')

df_train['Retinopathy grade'] = df_train['Retinopathy grade'].replace([0,1,2,3,4],[0,0,1,1,1])
df_train[['Retinopathy grade']].hist(figsize = (10, 5))
df_train = df_train.sample(frac=1).reset_index(drop=True)
plt.title('Groundtruth Labels for Diabetic Retinopathy after binarization')
plt.xlabel('Label')
plt.ylabel('Number of Images')

N_Training = round(len(df_train) * 0.8)
train = df_train[:N_Training]
validation = df_train[N_Training:]
print('---------------------------------------------------------')
print('Splitting the train samples into train and validation set')
print('---------------------------------------------------------')
print('Number of training samples:',len(train))
print('Number of validation samples:',len(validation))

label_0 = train[train['Retinopathy grade'] == 0]
label_1 = train[train['Retinopathy grade'] == 1]
print('Label 0:',len(label_0),'\n','Label 1:',len(label_1))

label_count_1, label_count_0 = train['Retinopathy grade'].value_counts()
label_0 = label_0.sample(label_count_1,replace=True)
df_train_sampled = pd.concat([label_0,label_1])
df_train_sampled = df_train_sampled.sample(frac=1,random_state=0)
print(len(label_0),len(label_1))
#print(df_train_sampled)
#df_train_sampled[['Retinopathy grade']].hist(figsize = (10, 5))

df_train_sampled['Image name'] = df_train_sampled['Image name'] + '.jpg'
validation['Image name'] = validation['Image name'] + '.jpg'

train_images_list = []
train_labels_list = []
for tname, tclass in df_train_sampled.itertuples(index=False):
    for ft in train_images:
      if os.path.basename(ft) == tname:
        #print(fp,iname,iclass)
        train_images_list.append(ft)
        train_labels_list.append(tclass)

val_images_list = []
val_labels_list = []
for vname, vclass in validation.itertuples(index=False):
  for fv in train_images:
      if os.path.basename(fv) == vname:
        #print(fv,vname,vclass)
        val_images_list.append(fv)
        val_labels_list.append(vclass)

val_img = np.array([img_to_array(load_img(img, target_size=(256, 256)))for img in val_images_list]).astype('float32')
val_labels_list = np.array(val_labels_list)

print(len(train_images_list),len(train_labels_list))
print(len(val_images_list), len(val_labels_list))

def _parse_function(image, label):
  img_train = tf.io.read_file(image)
  img_decoded = tf.io.decode_jpeg(img_train)
  img_cropped = tf.image.central_crop(img_decoded, central_fraction=0.95)
  img_cropped_bound = tf.image.crop_to_bounding_box(img_cropped, 0 , 0 , target_height = 2700, target_width = 3580)
  image_cast = tf.cast(img_cropped_bound, tf.float32) 
  image_cast = image_cast / 255.0
  image_resized = tf.image.resize(image_cast,size=(img_ht,img_wd))
  return image_resized, label

def build_dataset(images, labels):
  AUTOTUNE = tf.data.experimental.AUTOTUNE
  dataset = tf.data.Dataset.from_tensor_slices((images, labels))
  dataset = dataset.cache()
  dataset = dataset.map(_parse_function)
  dataset = dataset.batch(len(images))
  dataset = dataset.prefetch(AUTOTUNE)
  return dataset

train_dataset = build_dataset(train_images_list, train_labels_list)
#Debug
print(tf.data.experimental.cardinality(train_dataset).numpy())

def to_train_datagen():
  for image, label in train_dataset:
    image_matrix = image.numpy()
    label_matrix = label.numpy()
    print(image_matrix.shape)
    print(label_matrix.shape)

  return image_matrix, label_matrix

train_image_array, train_label_array = to_train_datagen()

# Create train generator.
train_datagen = ImageDataGenerator(rotation_range=30, 
                                   width_shift_range=0.2,
                                   height_shift_range=0.2, 
                                   horizontal_flip = 'true')
train_generator = train_datagen.flow(train_image_array, train_label_array, shuffle=False, batch_size=batch_size)

# Create validation generator
val_datagen = ImageDataGenerator(rescale = 1./255)
val_generator = val_datagen.flow(val_img, val_labels_list, shuffle=False, 
                                   batch_size=batch_size)

from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras import optimizers

model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', 
                 input_shape=(256,256,3)))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax'))

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

print(model.summary())

epochs=200
history = model.fit(
  train_generator,
  validation_data=val_generator,
  epochs=epochs
)

model.save('DRD_Moel_Team_05.h5')

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(25, 9))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

#Prediction
#Loading test images 
t = []
test_images = glob.glob("/content/drive/MyDrive/IDRID_dataset/images/test/*.jpg")
#print(len(test_images))
df_test = pd.read_csv('/content/drive/MyDrive/IDRID_dataset/labels/test.csv')
df_test['Retinopathy grade'] = df_test['Retinopathy grade'].replace([0,1,2,3,4],[0,0,1,1,1])
df_test['Image name'] = df_test['Image name'] + '.jpg'
df_test = df_test.drop_duplicates()
df_test = df_test.iloc[:, : 2]
#print(df_test)
predicted_label_list = []
for iname,iclass in df_test.itertuples(index=False):
    for file in test_images:
      if os.path.basename(file) == iname:
        img = tf.io.read_file(file)
        img = tf.io.decode_jpeg(img)
        img = tf.cast(img,tf.float32) / 255
        img = tf.image.resize_with_pad(img,img_ht,img_wd,antialias=True)
        img = tf.reshape(img, [1,256,256,3])
        t.append(img)
        x = model.predict(img)
        predicted_label = np.argmax(x)
        predicted_label_list.append(predicted_label)

df_test['Predicted Class'] = predicted_label_list
df_test['Result'] = np.where(df_test['Retinopathy grade'] == df_test['Predicted Class'], 'Correct Prediction', 'Incorrect Prediction')
#print(df_test)

df_test['Result'].value_counts()

from sklearn import metrics
from sklearn.metrics import confusion_matrix
import seaborn as sb
cm = confusion_matrix(df_test['Retinopathy grade'],df_test['Predicted Class'])
plt.figure(figsize = (10,5))
plt.title('Confusion Matrix')
sb.heatmap(cm, cmap="Blues", annot=True,annot_kws={"size": 16})
print('Test Accuracy:',metrics.accuracy_score(df_test['Retinopathy grade'], df_test['Predicted Class']))

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve

auc = roc_auc_score(df_test['Retinopathy grade'], df_test['Predicted Class'])
print('AUC: %.2f' % auc)


fpr, tpr, thresholds = roc_curve(df_test['Retinopathy grade'], df_test['Predicted Class'])
# create plot
plt.plot(fpr, tpr, label='ROC curve')
plt.plot([0, 1], [0, 1], 'k--', label='Random guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.legend(loc="lower right")

from tensorflow.keras.models import Model
import tensorflow as tf
import numpy as np
import cv2
import imutils
from google.colab.patches import cv2_imshow

def compute_heatmap(image, eps=1e-8):
  gradModel = tf.keras.models.Model([model.input], [model.get_layer('conv2d_3').output, model.output])
  
  with tf.GradientTape() as tape:
    inputs = tf.cast(image, tf.float32)
    (convOutputs, predictions) = gradModel(inputs)
    loss = predictions[:, 1]
    grads = tape.gradient(loss, convOutputs)

  castConvOutputs = tf.cast(convOutputs > 0, "float32")
  castGrads = tf.cast(grads > 0, "float32")
  guidedGrads = castConvOutputs * castGrads * grads
  convOutputs = convOutputs[0]
  guidedGrads = guidedGrads[0]
  weights = tf.reduce_mean(guidedGrads, axis=(0, 1))
  cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)
  
  (w, h) = (image.shape[2], image.shape[1])
  heatmap = cv2.resize(cam.numpy(), (w, h))
  
  numer = heatmap - np.min(heatmap)
  denom = (heatmap.max() - heatmap.min()) 
  heatmap = numer / denom
  heatmap = (heatmap * 255).astype("uint8")
  print(heatmap)

  return heatmap

def overlay_heatmap(heatmap, image, alpha=0.5,
		colormap=cv2.COLORMAP_VIRIDIS):
  heatmap = cv2.applyColorMap(heatmap, colormap)
  output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)

  return (heatmap, output)

orig = cv2.imread('/content/drive/MyDrive/IDRID_dataset/images/test/IDRiD_004.jpg')
resized = cv2.resize(orig, (256, 256))
print(resized.shape)
image = load_img('/content/drive/MyDrive/IDRID_dataset/images/test/IDRiD_004.jpg', target_size=(256, 256))
image = img_to_array(image)
image = np.expand_dims(image, axis=0)
print(image.shape)

preds = model.predict(image)
i = np.argmax(preds[0])
print(i)


heatmap = compute_heatmap(image)

heatmap = cv2.resize(heatmap, (orig.shape[1], orig.shape[0]))
(heatmap, output) = overlay_heatmap(heatmap, orig, alpha=0.5)

cv2.rectangle(output, (0, 0), (340, 40), (0, 0, 0), -1)
cv2.putText(output, '1', (10, 25), cv2.FONT_HERSHEY_SIMPLEX,0.8, (255, 255, 255), 2)

output = np.vstack([orig, heatmap, output])
output = imutils.resize(output, height=500)
cv2_imshow(output)
cv2.waitKey(0)
